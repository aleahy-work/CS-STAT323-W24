{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493b87ef-67b5-4b9e-aa8f-f25ea864ae92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Some Scikit-Learn Problems #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a980083-7e87-4cbc-98ec-3299f47aa30c",
   "metadata": {},
   "source": [
    "### Problem 1: SKLearn's circle dataset ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78920cba-8630-4757-babb-b2d9ca5e58fa",
   "metadata": {},
   "source": [
    "The following code uses the sklearn [circle data set generator](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html) to produce a data set with a certain distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e57b621c-7524-4438-8af2-55c7286aa93a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "circX, circy = make_circles(200, factor=0.3, noise=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7c2859-4724-4974-aa31-707b418dbc90",
   "metadata": {},
   "source": [
    "+ Produce a plot of the data.  Then use logistic regression to train a model to predict the category 'circy' as a function of the coordinates 'circX'. Finally, use the same code above to produce a test data set. Find the accuracy score of the model on the test dataset.\n",
    "\n",
    "+ Do the same problem with an SVM classifier.  (Note the default kernel function for SVC is a *radial basis function*.  See the next problem.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd1c4d-18ac-4fc0-81b2-ad8a14872d2c",
   "metadata": {},
   "source": [
    "### Problem 2: Plotting Decision Boundaries ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7b5ae-f919-48b3-8b09-3b4dc619e392",
   "metadata": {},
   "source": [
    "This problem will use the SKLearn [DisplayDecisionBoundary](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html) command to visualize the regions in the plane that SKLearn classifiers will assign to each category.  It assumes you have watched the [decisionboundary.mp4](https://drive.google.com/file/d/1Z6qKNkNylFfFB2kG-O5QwjfXPUcckIhp/view?usp=drive_link) video for an introduction to the command.  It will also make use of the 01-25prob1.csv data file on [the class github page](https://github.com/aleahy-work/CS-STAT323-W24).\n",
    "\n",
    "The problem will look at different kernel functions used in SVC to get a sense of their capabilities. These kernels $K({\\bf x}_i, {\\bf x}_j) = \\Phi({\\bf x}_i) \\cdot \\Phi({\\bf x}_j)$ are incorporated into function to be optimized using lagrange multipliers\n",
    "\n",
    "$\n",
    "  L_p = \\sum_{N_s} \\alpha_i y_i \\Phi({\\bf x}_i) \\cdot \\Phi({\\bf x}_j) + b\n",
    "$\n",
    "    \n",
    "\n",
    "   \n",
    "essentially move the problem into higher-dimensional space where linear separation becomes possible.  Practically speaking, they can dramatically impact the shape of the decision boundary. There are [several different kernel functions](https://scikit-learn.org/stable/modules/svm.html#kernel-functions) supported by the [SKLearn SVC class](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), most of which depend on various parameters.  Our goal is to find an appropriate kernel function for the data. \n",
    "\n",
    "+ Produce a scatter plot of the data and determine whether a linear classifier such as perceptron or adaline is appropriate.  Explain.\n",
    "\n",
    "+ The simplest kernel function is the **linear** kernel function, which is the original dot product ${\\bf x}_1 \\cdot {\\bf x}_2$.  This is the 'trivial' kernel function, which means it doesn't change anything.  **However**, it is not the default in the SVC class.  It has to be specified with the kernel= parameter.  See the [SVC link](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for how to do this, and produce a plot showing (1) the decision boundary and (2) plot of the data, immitating the video linked above.\n",
    "\n",
    "+ Another kernel is the **polynomial** kernel $({\\bf x}_1 \\cdot {\\bf x}_2 + r)^d$, which depends on the parameters r and d.  Play around with these parameters to find a model that seems the best for your data and produce a plot of the decision boundaries.  See the coef0 and degree parameters in the [SVC class](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). \n",
    "\n",
    "+ One of the most important kernels is the **radial basis kernel** $exp(- \\|{\\bf x_1} - {\\bf x_2}\\|^2)$.  Produce a plot of the decision boundaries for this kernel together with our datasets.\n",
    "\n",
    "+ *Regularization* is used to improve many machine learning models. Essentially, it modifies the loss function to make the decision boundaries smoother to avoid overfitting.  In the SVC class, the regularization parameter is given by C and defaults to $C = 1$.  (See the documentation page for details.)  Try different values of C with your *radial basis model* to find a value of C which gives the best fit to your train data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a869b-131d-4a62-b9f3-05ef747d64bf",
   "metadata": {},
   "source": [
    "### Problem 3: Fashion MNIST ### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74137d9-37c4-4f95-9227-58cc0700bbec",
   "metadata": {},
   "source": [
    "The [fashionMNIST database](https://github.com/zalandoresearch/fashion-mnist/tree/master) is a collection of 70,000 28x28 black and white images consisting of ten different categories of clothing taken from an online catalog.  The 10 categories and their numerical label are given below: \n",
    "\n",
    "| Label | Description |\n",
    "| --- | ----|\n",
    "|0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2\t| Pullover |\n",
    "| 3\t| Dress |\n",
    "| 4\t| Coat | \n",
    "| 5\t| Sandal | \n",
    "| 6\t| Shirt | \n",
    "| 7\t| Sneaker | \n",
    "| 8\t| Bag | \n",
    "| 9 | Ankle boot |\n",
    "\n",
    "Your goal in this problem is to develop a model which classifies these items.  \n",
    "\n",
    "**Warning:** Downloading the data is a multi-step process.  The [data itself](https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion) is available on Github.  There are four files, divided into train vs test (\"t10k\") and into images and labels.  For instance, t10k-images-idx3-ubyte.gz refers to the 10,000 test images and t10k-labels-idx1-ubyte.gz references to the corresponding labels.  You will need to\n",
    "\n",
    "+ Download each of these gzipped files into a directory that you can access through Jupyter.  (**Note:** We'll only use the train files, generating our own test dataset.) Then evaluate the cell below to load the python load_mnist() function and use it to load the **train** data using the function call in the next cell.   Confirm that the data is loaded using the \".shape\" attribute for your images and your labels. What do the dimensions mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7165d0b0-71e1-4581-aaed-59f211dab230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f7e161-b3d8-4574-ac00-f9de7aab2432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Assuming you downloaded the data to a folder called 'fashion-mnist', which is a\n",
    "subfolder of the folder you are currently working with, you can load\n",
    "the 'train' images and classes with the following function call.\n",
    "\"\"\"\n",
    "myX, myy = load_mnist('fashion-mnist','train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefbf9f6-0efe-4fa3-a34d-e9ec6f14e670",
   "metadata": {},
   "source": [
    "+ Produce an image of the 10th item in the train image dataset. (Remember to count from zero.)  What category does the data belong to (number and name)?   **Note:** Each image is in a linear array (not a 2-dimensional array--as images are supposed to be.  You will need to reshape the data for the image, **but** it shouldn't be reshaped permanently, because the model will expect each image to be a linear array. \n",
    "\n",
    "+ In the following you will be developing a model to classify the 'shirt' category vs the 'sneaker' category. Create a subset of the train images and labels consisting only of shirts and sneakers.  What are the sizes of the resulting arrays (images and labels)?  In particular, how many images are contained in this subset?\n",
    "\n",
    "+ You can produce a random set of integers within a given range using the numpy [randint function](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html).  Use this to select a subset of 6 images from the shirts and sneakers dataset.  Produce a plot of all of these 6 images together.  Use the matplotlib [subplots function](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html) to arrange these 6 images in a 2 (row) by 3 (column) grid. (See the link for suggestions on how to use mathplotlib's plt.subplots() to do this.)\n",
    "\n",
    "**Warning:** With 60K entries, this dataset is often too big for most of the classification techniques used in SKLearn.  (The model can \".fit\" the data, but I've found the training time can be very long for large datasets.)  At the risk of hanging your notebook, don't try this on the full dataset.\n",
    "\n",
    "+ Write a function sample( X, y, n) that returns newX and newY, a sample of size n from the X (images) and y (labels).  \n",
    "\n",
    "+ Select a random sample of 50 items from the shirt and sneaker dataset and fit a SVM model for predicting whether something is a shirt or sneaker image. Select another sample of size 50 as a test dataset and determine the accuracy score for this model. \n",
    "\n",
    "+ Write a function testmodel( X, y, n) that (1) produces samples of size n for X and y, (2) trains a model on this data, (3) selects a test sample of size n, and (4) returns the accuracy score for the model.  Use it to train the model with samples of size 100, 150, etc, until your systems starts to get slow. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
