{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f9c665-6b73-4c35-b5c6-2ee74f411b1a",
   "metadata": {},
   "source": [
    "# Some K-Means Clustering Problems #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634cd58-e6a6-40b8-994c-adb6a36fd133",
   "metadata": {},
   "source": [
    "## Problem 1 ##\n",
    "This problem has to do with extending the k-Means clustering class (from [here](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)) which was discussed in class and is reproduced below.  The key method in the class is (as usual) \".fit()\", which establishes centroids for each cluster and labels for each point in the data set--i.e., which cluster each point belongs to.\n",
    "\n",
    "Your task in this problem will be to modify the class to produce additional output. \n",
    "\n",
    "**Note:** The k-means clustering algorithm is *not* guaranteed to converge, so you may not always get the outcome you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42fce9a6-3f8c-4494-b0f7-b0fc63d87566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class Kmeans:\n",
    "    '''Implementing Kmeans algorithm.'''\n",
    "\n",
    "    def __init__(self, n_clusters, max_iter=100, random_state=123):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def initializ_centroids(self, X):\n",
    "        np.random.RandomState(self.random_state)\n",
    "        random_idx = np.random.permutation(X.shape[0])\n",
    "        centroids = X[random_idx[:self.n_clusters]]\n",
    "        return centroids\n",
    "\n",
    "    def compute_centroids(self, X, labels):\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        for k in range(self.n_clusters):\n",
    "            centroids[k, :] = np.mean(X[labels == k, :], axis=0)\n",
    "        return centroids\n",
    "\n",
    "    def compute_distance(self, X, centroids):\n",
    "        distance = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            row_norm = norm(X - centroids[k, :], axis=1)\n",
    "            distance[:, k] = np.square(row_norm)\n",
    "        return distance\n",
    "\n",
    "    def find_closest_cluster(self, distance):\n",
    "        return np.argmin(distance, axis=1)\n",
    "\n",
    "    def compute_sse(self, X, labels, centroids):\n",
    "        distance = np.zeros(X.shape[0])\n",
    "        for k in range(self.n_clusters):\n",
    "            distance[labels == k] = norm(X[labels == k] - centroids[k], axis=1)\n",
    "        return np.sum(np.square(distance))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.centroids = self.initializ_centroids(X)\n",
    "        for i in range(self.max_iter):\n",
    "            old_centroids = self.centroids\n",
    "            distance = self.compute_distance(X, old_centroids)\n",
    "            self.labels = self.find_closest_cluster(distance)\n",
    "            self.centroids = self.compute_centroids(X, self.labels)\n",
    "            if np.all(old_centroids == self.centroids):\n",
    "                break\n",
    "        self.error = self.compute_sse(X, self.labels, self.centroids)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distance = self.compute_distance(X, self.centroids)\n",
    "        return self.find_closest_cluster(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec46035-283c-4cb9-a8a8-8c5ce1e35d6d",
   "metadata": {},
   "source": [
    "1. Consider the dataset 02-02prob1.csv with two feature variables and available on the github page. Produce a scatter plot of the data and visually estimate the number of clusters in the datset.\n",
    "\n",
    "2. Consider the class Kmeans defined above, and in particular its \".fit()\" method.  Some questions to answer:\n",
    "+ What are the parameters for the class that can be changed on initialization?\n",
    "+ What are the stopping criteria (note plural) for the method?\n",
    "+ What are the three attributes that the method produces/updates?  Give a description of each.\n",
    "\n",
    "3. The class is very general, and can work with clusters in arbitrary dimensions.  But, as written, the class consists primarily of internal methods and, once fitted, can only be used to predict which cluster a new value of the feature variables belongs to (using \".predict()\").  Write a method  .print_centroids()for the class which returns the centroids.\n",
    "\n",
    "4. Write another method .display_cluster() which (1) produces a scatterplot of the clusters with (2) the color of each point indicating the cluster it belongs to and (3) the centroids of each cluster also displayed in **black**.\n",
    "\n",
    "5. The k-means clustering algorithm progresses by randomly choosing starting centroids from among the points and then alternatively labelling data points according to the nearest centroid and then updating the centroids to reflect these new clusters--repeating these two steps until a stopping criterion is met.  Write a method .display_fit() for the class which finds the clusters as in the \".fit()\" method, but also produces a scatterplot like the one in the previous part *for each iteration* of the algorithm--the goal being to visually display how the cluster classification progresses as the algorithm progresses through each step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b07d7-66d0-40d4-b708-e98f07c41e53",
   "metadata": {},
   "source": [
    "## Problem  2 ##\n",
    "\n",
    "The k-means clustering algorithm assumes that the number $k$ of clusters is already known.  But for high-dimensional data, it may not be easy to determine $k$.  One technique for estimating the number of clusters (see p. 313 of the textbook) is the \"elbow method\".   \n",
    "\n",
    "The idea is that the \"SSE error\", which gives the total sum of squares of distances from each point in the data to the centroid of its cluster (see the Class definition above), can be used to estimate the number of centers as follows: For each choice in a range of possible values of $k$, plot the SSE produced by the k-means algorithm for that $k$.  The actual number of clusters should be the $k$ which occurs at the 'elbow' of that plot.  Why?  Geometrically, if a centroid is located at the center of its actual cluster, it should be almost as small as possible. When points from other clusters are included in the cluster (e.g., when $k$ is too small), the SSE will be too big, because the centroid is associated with points that are too far away.  When $k$ is too big, the SSE will be smaller but not by much, since you are simply dividing clusters into smaller pieces and SSEs between the unneeded clusters won't be significantly different.  \n",
    "\n",
    "See [this link](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html) for a discussion of how this can be implemented with SKLearn.\n",
    "\n",
    "**Your assignment:**\n",
    "\n",
    "1. Consider the data set 02-02prob2.csv.  This is a blob of an unspecified number of clusters in 7-dimensional space.  Write a function elbow(n), which produces an elbow plot for the points (k,SSE(k)) for each k between 1 and n, and use it to estimate the number of clusters is my blob.  **The function should make use of the Kmeans class above**, which may need to be modified to produce the needed output.\n",
    "\n",
    "**Note:** The number of clusters in the blob may not be well-defined.  I've used the SKLearn blob function to create clusters which, when plotted, clearly result in fewer clusters than I've specified.  Hence the nature of randomness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510c5c5-1957-42bb-92ce-c9af1b546e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
