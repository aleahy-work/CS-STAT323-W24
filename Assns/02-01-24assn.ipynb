{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23cba6f6-9cee-4870-80b8-096cfc4d630b",
   "metadata": {},
   "source": [
    "# PCA and Text Models #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0470f087-6dba-44ad-b4ef-2a53563a634b",
   "metadata": {},
   "source": [
    "### Problem 1 ###\n",
    "\n",
    "Consider the two randomly-generated bivariate datasets defined by the following cell (which you should evaluate before proceeding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "594d7383-a822-4c5a-bc74-eef186b02635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data1 = np.random.multivariate_normal([0,0], [[1,0],[0,1]], 100)\n",
    "data2 = np.random.multivariate_normal([0,0], [[1,-.9],[-.9,1]], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25be597-a66d-400d-bd5d-5714e9e460fa",
   "metadata": {},
   "source": [
    "1. Produce two side-by-side scatterplots, one for each data set. Just from the graph, what would you say about the correlation between the $x$ and $y$ variables in each case?  Explain.\n",
    "   \n",
    "2. For data1, what is the percentage of total variability explained by the first principal component?\n",
    "\n",
    "3. Given the scatterplot for data2, would you predict that the percentage of total variability for data2 is higher or lower (or about the same) for the first principal component in data2? Explain your reasoning. (Mentally visualize the first principal component on the plot if it helps.)\n",
    "\n",
    "4. Find the percentage of total variability explained by the first principal component for data2.\n",
    "\n",
    "5. What is the first principle component for data2?  Produce a plot of a line containing this first principle component together with the scatterplot for data2.  (Recall that vectors are just line segments beginning at zero and ending at the tip of the vector.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955042f8-4d1e-4162-878f-d3d062c653ed",
   "metadata": {},
   "source": [
    "## Problem 2 ##\n",
    "\n",
    "Consider the 'imdb-dataset.csv' file in your CNotesCS323_1 folder on euclid.\n",
    "\n",
    "1. Use the techniques learned in class to clean up the text in the *entire* 'reviews' column of this data set. To test and evaluate your clean-up algorithm, select a *random* sample of **6** of the 50000 cleaned-up reviews and display them in your solution\n",
    "\n",
    "**Warning:**  I had little success in implementing stop-words, stemming, etc.  In general, any technique that did a [list comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp) like:  \n",
    "\n",
    " [word for word in mywords if word ...]\n",
    "\n",
    "on the 50000 reviews took too long, so I shut it down. You may have more success.\n",
    "\n",
    "2. How many columns are in the term frequency count vector produced by your cleaned up text?  (The best I was able to do was 100312 using the techniques from class. You may do better!)\n",
    "\n",
    "3. As done in class (and the text) produce a train and test dataset consisting of the first- and second-half of the data.  Then evaluate it using one of the classification models in SKLearn.  What is your accuracy score?  (The best I obtained was about 89%.)\n",
    "\n",
    "**Warning:**  I only had success with Perceptron and LogisticRegression.  Some models can't handle the sparse data format; other models aapparently can't handle a large number of data points (50,000) with a large number of indices (100000).  SKLearn has [some built-in strategies](https://scikit-learn.org/stable/computing/scaling_strategies.html) for dealing with large datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810f39e9-6040-413b-9463-d629f52ccdf0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Problem 3 ##\n",
    "\n",
    "The [Wisconsin breast cancer database](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) is another well-known dataset in machine learning.  It consists of 30 feature variables \"from a digitized image of a fine needle aspirate (FNA) of a breast mass\"  which \"describe characteristics of the cell nuclei present in the image\".  The predicted variable is whether the lump is 'benign' or 'malignant'.  More details are described at the link and its references. \n",
    "\n",
    "The data can be loaded using a [sklearn method](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer).  Note the data is returned in a 'bunch' format, which you will have to understand, though the data can ultimately be returned as a pandas dataframe. \n",
    "\n",
    "1. Download and split the data into a train and test set consisting of 80% training data. THEN compute mean and variance of each feature column. \n",
    "\n",
    "2. Use a 5-NearestNeighbor algorithm to predict malignancy from the 30 feature variables. Produce an accuracy score for your model using the test data.\n",
    "\n",
    "3. Perform a PCA on the feature variables to determine the number of principal components necessary to explain 90% of the total variability.  As an explanation, produce a plot of the cumulative explained variability. \n",
    "\n",
    "**Note:** If your answer is \"1 principal component\", read [this discussion](https://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-principal-component-analysis-pca).  You haven't encountered this issue before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9b25d8-bd66-4c80-abbd-0b9ae2cd0703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
